<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.min.js"></script>

  <title>
    Small Area Estimation in R - Part 1 &middot; 
    Geographic Data Science Laboratory
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-gdsl">

    <header class="masthead">
      <div class="masthead-inner">
			 <div class="img-circular"></div>
        <h1>Geographic Data Science Laboratory</h1>
        <p class="lead">Geographic Data Science Laboratory at the University of Liverpool</p>

        <div class="colophon">
          <ul class="colophon-links">
             <li>
              <a href="/about">About</a>
            </li>
             <li>
              <a href="/">Blog</a>
            </li>
              <li>
              <a href="http://atlas.geographicdatascience.org">Atlas</a>
            </li>
           <li>
              <a href="/people/">People</a>
            </li>
             <li>
              <a href="/projects/">Our Projects</a>
            </li>
            <li>
              <a href="/join/">Join Us</a>
            </li>
             <li>
              <a href="/contact/">Contact Us</a>
            </li>
          </ul>
          <p>&copy; 2014. Geographic Data Science Laboratory All rights reserved.</p>
          <a href="/atom.xml"><img class="img-feed" src="/public/images/iconmonstr-rss-3-icon.svg"></a></img><a href="https://twitter.com/geodatascience"><img class="img-feed" src="/public/images/iconmonstr-twitter-3-icon.svg"></img></a><a href="https://github.com/alexsingleton/geographicdatascience"><img class="img-feed" src="/public/images/iconmonstr-github-8-icon.svg"></a></br>
         <a href="http://www.liv.ac.uk"><img class="img-uni" src="/public/images/whiteout_logo_616.png"></a> 
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="post">
  <h1>Small Area Estimation in R - Part 1</h1>
      <span class="post-date">
    	<img class="img-icon" src="/public/images/iconmonstr-calendar-5-icon.svg"></img>24 Jan 2014
   		<img class="img-icon" src="/public/images/iconmonstr-user-icon.svg"></img>Michail Pavlis
   		<img class="img-icon" src="/public/images/iconmonstr-tags-3-icon.svg"></img> r  
    	<img class="img-icon" src="/public/images/iconmonstr-speech-bubble-14-icon.svg"></img><a href="http://www.geographicdatascience.com/r/2014/01/24/small-area-estimation-in-r-pt1/#disqus_thread" data-disqus-identifier="/r/2014/01/24/small-area-estimation-in-r-pt1/"></a>
	    <img class="img-icon" src="/public/images/iconmonstr-twitter-icon.svg"><a href="https://twitter.com/share?url=http://www.geographicdatascience.com/r/2014/01/24/small-area-estimation-in-r-pt1/&text=Small Area Estimation in R - Part 1" target="_blank">Tweet this!</a>
	  </span>
  <p>A substantial part of my work has to do with statistics and mostly in the area of geodemographics and <a href="http://en.wikipedia.org/wiki/Small_area_estimation">Small Area Estimation</a> (SAE). Basically what I am trying to do is to update the <a href="http://areaclassification.org.uk/getting-started/getting-started-what-is-the-output-area-classification/">Output Area Classification</a> (OAC) for England by producing intercensal estimates of the OAC variables.  Some of these estimates are already available, such as population estimates, but for the most part we have to create statistical models using secondary data sources such as school data, council tax bands, indices of deprivation and so on. <!-- more --></p>

<h2>Background</h2>

<p>There are two major distinctions in regards with the statistical methods used in SAE, the direct and indirect estimators. The former makes use of domain-specific data and design-based weights while the latter  mostly rely on the use of auxiliary data. Further distinctions of indirect estimators can be made by identifying whether they “borrow strength” from a different domain, time or a combination of the two. Indirect estimators are also called model-based estimators and use techniques such as mixed-effects models and the empirical best linear unbiased prediction (EBLUP), hierarchical Bayes and empirical Bayes. A wealth of information on SAE can be found in the book Small Area Estimation by Rao.</p>

<p>Mixed-effects models are applied to data which are nested (also referred to as hierarchical or multilevel). Geodemographic data are nested because there is a spatial hierarchy, for example output areas are within lower super output areas. This hierarchy can be included in the random part of the model while the fixed part of the model consists of the explanatory variables. More information on mixed-effects models can be found <a href="http://zoonek2.free.fr/UNIX/48_R/14.html">here</a>.</p>

<p>My own background is in the area of environmental sciences and I had the opportunity to use mixed-effects models, relying mostly on the excellent book by Zuur et al “Mixed effects Models and extentions in ecology using R”.  I have used in the past several packages in R to develop mixed effects models, including nlme, mgcv and lme4. There are several options for Bayesian analysis in R as well, but what I quickly found out was that running a Bayesian model at the national scale can take an enormous amount of time. This makes difficult to test and validate these models so in my case it was impossible to consider using Bayesian models.</p>

<h2>Example</h2>

<p>The type of model that I used was dictated by the type of data that I was interested to predict. Most of the variables that were used to develop the OAC were proportional data and therefore a generalized mixed-effects model (GLMM) with a binomial distribution had to be applied for their prediction (having said that, it has been suggested that a linear mixed effects model sometimes can provide better results, see for example this <a href="http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0CDQQFjAC&amp;url=http%3A%2F%2Fwww.researchgate.net%2Fpublication%2F256972075_Use_of_Spatial_Information_in_Small_Area_Models_for_Unemployment_Rate_Estimation_at_Sub-Provincial_Areas_in_Italy%2Ffile%2F504635241a2a1c8159.pdf&amp;ei=fsvnUqWSENSB7Qab-oHYCw&amp;usg=AFQjCNECEufo24VHT_f3AYyJi9I1JfvLoA&amp;bvm=bv.59930103,d.ZGU">paper</a>). lme4 is a good package to start exploring GLMM models in R and I will provide an example of using lme4 later in this post. Aspects of the analysis that one should pay particular attention include:
  * Exploratory Data Analysis (identification of suitable predictors, outliers, non-linearity).
  * Model Selection for the fixed part and the random part of the regression.
  * Internal validation (based on the residuals).
  * Overdispersion (i.e. the variation in the data exceeds the expected variability based on the binomial distribution assumptions).</p>

<p>If you want to reproduce the analysis you will have to download the 2001 Census data on <a href="http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&amp;Function=&amp;%24ph=60_61&amp;CurrentPageId=61&amp;step=2&amp;datasetFamilyId=41&amp;instanceSelection=121&amp;Next.x=16&amp;Next.y=5">Routine/Semiroutine Occupation</a>. So download the files for the following regions: East Midlands, East of England, London, North East, North West, South East, South West, West England and Yorkhire (9 files in total). Select csv as file type, download the zip file in a folder and then unzip the file ending in "_OA.CSV". The following R script will select the appropriate columns, merge the data and save them in a new csv file.</p>

<pre><code class="r">cols = c(12, 15, 28, 30)
setwd("path/to/census_data")  # Provide the path to Census 2001 csv files
csv_files = list.files(".", ".CSV")
temp &lt;- NULL
for (i in csv_files) {
    pop &lt;- read.csv(i, skip = 5, header = T)
    pop &lt;- pop[, cols]
    names(pop) &lt;- c("oa", "all_people", "semi_routine_occupation", "routine_occupation")
    temp &lt;- rbind(temp, pop)
}
out &lt;- data.frame(oa_01 = temp[, 1], all_people_01 = temp[, 2])
out$routine_count_01 &lt;- rowSums(temp[, 3:4])
out$routine_perc_01 &lt;- rowSums(temp[, 3:4])/temp[, 2]

write.csv(out, "occupation_2001.csv", row.names = F)
</code></pre>

<p>Consequently download the <a href="https://geoportal.statistics.gov.uk/Docs/Lookups/Output_areas_(2001)_to_lower_layer_super_output_areas_(2001)_to_middle_layer_super_output_areas_(2001)_E+W_lookup.zip">lookup table</a> between OA 2001 and LSOA and unzip the csv file. You will also need the Indices of Deprivation 2004 (Education), so go ahead and download it <a href="http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&amp;Function=&amp;%24ph=60_61&amp;CurrentPageId=61&amp;step=2&amp;datasetFamilyId=2088&amp;instanceSelection=124670&amp;Next.x=13&amp;Next.y=18">here</a>. Again select csv as data type and from the zipped file export the file ending to "_LSOA.CSV". We will also need the council tax bands data for 2001, which you can find <a href="http://www.neighbourhood.statistics.gov.uk/dissemination/datasetList.do?JSAllowed=true&amp;Function=&amp;%24ph=60&amp;CurrentPageId=60&amp;step=1&amp;CurrentTreeIndex=-2&amp;searchString=tax+bands&amp;datasetFamilyId=938&amp;Next.x=13&amp;Next.y=7">here</a>. Like the 2001 census data, tax bands are available for each region so save them in a new folder and then merge them to a single file using the following script.</p>

<pre><code class="r">setwd("path/to/tax_bands_data")  # Provide the path to tax bands data
csv_files &lt;- list.files(".", "*.CSV")
temp &lt;- NULL
cols &lt;- c(12, 15, 16, 18, 20, 22, 24, 26, 28, 30)
for (i in csv_files) {
    read_csv &lt;- read.csv(i, skip = 5, header = T)[, cols]
    names(read_csv) &lt;- c("oa_01", "all_bands", "band_a", "band_b", "band_c", 
        "band_d", "band_e", "band_f", "band_g", "band_h")
    temp &lt;- rbind(temp, read_csv)
}

write.csv(temp, "tax_bands_2001.csv", row.names = F)
</code></pre>

<p>Finally we can put all the data together and start with the analysis.</p>

<pre><code class="r">library(lme4)
source(file = "http://www.highstat.com/Book2/HighstatLibV6.R")
</code></pre>

<pre><code class="r">census_01 &lt;- read.csv("path/to/occupation_2001.csv")[, 1:4]

lookup &lt;- read.csv("path/to/OA01_LSOA01_MSOA01_EW_LU.csv")[, 1:2]

imd_education &lt;- read.csv("path/to/IMD_Education_2004.csv", skip = 5)[, c(9,14)]
names(imd_education) &lt;- c("LSOA01CD", "rank_education")

bands_01 &lt;- read.csv("path/to/tax_bands_2001.csv")
bands_01$a_perc &lt;- ifelse(bands_01$all_bands == 0, 0, bands_01$band_a/bands_01$all_bands)

census_01 &lt;- merge(census_01, lookup, by.x = "oa_01", by.y = "OA01CD")
all_data &lt;- merge(census_01, bands_01[, c("oa_01", "a_perc")], by.x = "oa_01", 
    by.y = "oa_01")
all_data &lt;- merge(all_data, imd_education[, c("LSOA01CD", "rank_education")], 
    by.x = "LSOA01CD", by.y = "LSOA01CD")

all_data$tr_a_perc &lt;- asin(sqrt(all_data$a_perc/(100 + 1))) + asin(sqrt((all_data$a_perc + 1)/(100 + 1)))

str(all_data)
# 'data.frame': 165665 obs. of 9 variables: 
# $ LSOA01CD : Factor w/ 34378 levels 'E01000001','E01000002',..: 1 1 1 1 1 1 1 1 2 2 ...  
# $ oa_01 : Factor w/ 165665 levels '00AAFA0001','00AAFA0002',..: 1 2 3 4 5 6 7 8 17
# $ all_people_01 : int 181 92 180 246 206 197 164 148 199 189 ...
# $ routine_count_01: int 6 0 3 3 0 7 9 3 0 6 ...  
# $ routine_perc_01 : num 0.0331 0 0.0167 0.0122 0 ...  
# $ a_perc : num 0 0 0 0 0 ...  
# $ rank_education : int 28215 28215 28215 28215 28215 28215 28215 28215 28452
# $ tr_a_perc : num 0.0997 0.0997 0.0997 0.0997 0.0997 ...
</code></pre>

<p>The response variable is routine_perc_01, while the explanatory variables are a_perc (Tax Band A) and rank_education. I should note that I prefer to use the Education Rank instead of Education Score as the former is ordinal data and we will have less problems with the regression analysis. I have also transformed the variable Band A using the Freeman - Tukey double arcsine transformation which is suitable for percentage data. You can see the effect of the transformation in the following QQ-plot, the main body of the data distribution is closer to normal.</p>

<p><img src="/public/images/qqplots.jpeg" alt="QQplot" /></p>

<p>You can create the QQ-plot as follows:</p>

<pre><code class="r">par(mfrow = c(1, 2))
for (i in c("a_perc", "tr_a_perc")) {
    qqnorm(all_data[[i]], main = i)
    qqline(all_data[[i]])
}
</code></pre>

<h3>Exploratory Data Analysis</h3>

<p>The next step is to perform EDA and we can start with looking at the correlations among the variables.</p>

<pre><code class="r">cor(all_data[,c(5,6:8)])
#                 routine_perc_01     a_perc rank_education  tr_a_perc
# routine_perc_01       1.0000000  0.5894138     -0.7334502  0.5974160
# a_perc                0.5894138  1.0000000     -0.6021874  0.9795349
# rank_education       -0.7334502 -0.6021874      1.0000000 -0.5994592
# tr_a_perc             0.5974160  0.9795349     -0.5994592  1.0000000
</code></pre>

<p>Education rank seems to be a good predictor and Band A is also related to the dependent variable.
We can check for outliers:</p>

<pre><code class="r">vars &lt;- c("a_perc", "tr_a_perc", "rank_education")
Mydotplot(all_data[, c(vars, "routine_perc_01")])
</code></pre>

<p><img src="/public/images/dotplot.jpeg" alt="dotplot" /></p>

<p>And visualize the relationship between the variables.</p>

<pre><code class="r">Myxyplot(all_data, vars, "routine_perc_01")
</code></pre>

<p><img src="/public/images/xyplot.jpeg" alt="xyplot" /></p>

<p>The functions Mydotplot and Myxyplot are available from the HighstatLibV6.R script that we imported earlier. Outliers do not appear to be a significant problem for our data and the relationships seem to be linear.</p>

<h3>Regression Analysis</h3>

<p>Before we start with regression analysis we need to create the response variable which should consist of two columns, the number of people working in a routine occupation and the number of people who don't. We will also use the LSOA code in the random part of the regression. We will first use the untransformed percentage of Band A. The code in R is as follows:</p>

<pre><code class="r">y &lt;- cbind(all_data$routine_count_01, all_data$all_people_01 - all_data$routine_count_01)

glm1 &lt;- glmer(y ~ a_perc + rank_education + (1 | LSOA01CD), data = all_data, 
    family = binomial)

summary(glm1)
# Generalized linear mixed model fit by maximum likelihood ['glmerMod']
# Family: binomial ( logit )
# Formula: y ~ a_perc + rank_education + (1 | LSOA01CD) 
#   Data: all_data 

#       AIC       BIC    logLik  deviance 
# 1428001.9 1428041.9 -713996.9 1427993.9 

# Random effects:
#  Groups   Name        Variance Std.Dev.
#  LSOA01CD (Intercept) 0.08882  0.298   
# Number of obs: 165665, groups: LSOA01CD, 32482

# Fixed effects:
#                  Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)    -9.541e-01  3.725e-03  -256.1   &lt;2e-16 ***
# a_perc          5.367e-01  2.492e-03   215.3   &lt;2e-16 ***
# rank_education -3.607e-05  1.912e-07  -188.6   &lt;2e-16 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# Correlation of Fixed Effects:
#             (Intr) a_perc
# a_perc      -0.404       
# rank_eductn -0.873  0.287
</code></pre>

<p>All the parameters are significant, the next step is to perform internal validation of the model.</p>

<pre><code class="r"># E1 &lt;- residuals(glm1, type="pearson")
# summary(E1)
#       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
# -14.200000  -1.044000  -0.062490  -0.007915   0.957100  18.520000 
# p1 &lt;- length(fixef(glm1)) + 1
# overdispersion1 &lt;- sum(E1 ^ 2) / (nrow(all_data) - p1)  # 2.615604, should be around 1
# F1 &lt;- fitted(glm1)
# summary(F1)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.01805 0.14610 0.20170 0.20960 0.26850 0.50770 
summary(all_data$routine_perc_01)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.0000  0.1366  0.2021  0.2094  0.2764  0.5666 
cor(all_data$routine_perc_01, F1)  # 0.8765911

par(mfrow = c(2, 1))
plot(F1, E1, xlab = "Fitted values", ylab = "Pearson residuals")
abline(h = 0, lty = 2)
plot(F1, all_data$routine_perc_01, xlab = "Fitted values", ylab = "Observed data")
abline(coef = c(0, 1), lty = 2)

all_data$E1 &lt;- E1
vars &lt;- c("a_perc", "rank_education")
Myxyplot(all_data, vars, "E1")
</code></pre>

<p><img src="/public/images/glm1_plot1.jpeg" alt="glm1_plot1" />
<img src="/public/images/glm1_plot2.jpeg" alt="glm1_plot2" /></p>

<p>Based on the internal validation, the model is overdispersed and we don't predict extremely high and low values (nevertheless we do predict within the physical range). Plotting the Pearson residuals against predicted values there is greater variance in the middle of the graph compared to the variance on the right of the graph. The correlation of the predicted values against the actual values seems to be good. The plot of Pearson residuals against Band A shows that we have greater variance at the ends of the plot, therefore a transformation of the variable might be useful. Rank education data against the residuals does not show any serious patterns.
So the next step is to repeat the regression analysis using the Band A transformed data.</p>

<pre><code class="r">glm2 &lt;- glmer(y ~ tr_a_perc + rank_education + (1 | LSOA01CD), data = all_data, 
    family = binomial)

summary(glm2)
# Generalized linear mixed model fit by maximum likelihood ['glmerMod']
#  Family: binomial ( logit )
# Formula: y ~ tr_a_perc + rank_education + (1 | LSOA01CD) 
#    Data: all_data 

#       AIC       BIC    logLik  deviance 
# 1429994.9 1430035.0 -714993.5 1429986.9 

# Random effects:
#  Groups   Name        Variance Std.Dev.
#  LSOA01CD (Intercept) 0.08436  0.2904  
# Number of obs: 165665, groups: LSOA01CD, 32482

# Fixed effects:
#                  Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)    -1.342e+00  4.744e-03  -282.9   &lt;2e-16 ***
# tr_a_perc       3.622e+00  1.722e-02   210.3   &lt;2e-16 ***
# rank_education -3.636e-05  1.870e-07  -194.5   &lt;2e-16 ***
---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# Correlation of Fixed Effects:
#             (Intr) tr__pr
# tr_a_perc   -0.713       
# rank_eductn -0.787  0.291
</code></pre>

<pre><code class="r">E2 &lt;- residuals(glm2, type = "pearson")
summary(E2)
#      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
# -14.200000  -1.050000  -0.063270  -0.007156   0.963700  18.810000 
p2 &lt;- length(fixef(glm2)) + 1
overdispersion2 &lt;- sum(E2 ^ 2) / (nrow(all_data) - p2)  # 2.638352
F2 &lt;- fitted(glm2)
summary(F2)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.01724 0.14590 0.20200 0.20960 0.26950 0.49210 
summary(all_data$routine_perc_01)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.0000  0.1366  0.2021  0.2094  0.2764  0.5666 
cor(all_data$routine_perc_01, F2)  # 0.8755668

par(mfrow = c(2, 1))
plot(F2, E2, xlab = "Fitted values", ylab = "Pearson residuals")
abline(h = 0, lty = 2)
plot(F2, all_data$routine_perc_01, xlab = "Fitted values", ylab = "Observed data")
abline(coef = c(0, 1), lty = 2)

all_data$E2 &lt;- E2
vars &lt;- c("tr_a_perc", "rank_education")
Myxyplot(all_data, vars, "E2")
</code></pre>

<p><img src="/public/images/glm2_plot1.jpeg" alt="glm2_plot1" />
<img src="/public/images/glm2_plot2.jpeg" alt="glm2_plot2" /></p>

<p>With regard to the second model, the use of the transformed variable appears to have slightly increased the AIC, so one could argue against transforming BAND A. Nevertheless, the plot of residuals against the transformed variable shows that heterogeneity was decreased so I decided to use the transformed variable. The final step of the analysis is to deal with overdispersion and to do this a random intercept for each entry was included in the model.</p>

<pre><code class="r">all_data$Eps &lt;- 1:nrow(all_data)

glm3 &lt;- glmer(y ~ tr_a_perc + rank_education + (1 | LSOA01CD) + (1 | Eps), data = all_data, 
    family = binomial)
summary(glm3)
# Generalized linear mixed model fit by maximum likelihood ['glmerMod']
#  Family: binomial ( logit )
# Formula: y ~ tr_a_perc + rank_education + (1 | LSOA01CD) + (1 | Eps) 
#    Data: all_data 

#       AIC       BIC    logLik  deviance 
# 1286541.7 1286591.8 -643265.9 1286531.7 

# Random effects:
#  Groups   Name        Variance Std.Dev.
#  Eps      (Intercept) 0.07189  0.2681  
#  LSOA01CD (Intercept) 0.06475  0.2545  
# Number of obs: 165665, groups: Eps, 165665; LSOA01CD, 32482

# Fixed effects:
#                  Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)    -1.263e+00  6.474e-03  -195.1   &lt;2e-16 ***
# tr_a_perc       3.156e+00  2.849e-02   110.8   &lt;2e-16 ***
# rank_education -3.806e-05  1.961e-07  -194.1   &lt;2e-16 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# Correlation of Fixed Effects:
#             (Intr) tr__pr
# tr_a_perc   -0.867       
# rank_eductn -0.783  0.462

E3 &lt;- residuals(glm3, type = "pearson")
summary(E3)
#      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
# -4.024000 -0.329100  0.001431 -0.039470  0.295900  2.959000 
p3 &lt;- length(fixef(glm3)) + 1
overdispersion3 &lt;- sum(E3 ^ 2) / (nrow(all_data) - p3)  # 0.2939502

all_data$E3 &lt;- E3
</code></pre>

<p>With that overdispersion for the last model is no longer a problem. The next part of the analysis is to predict new values of the response variable for the period 2002 - 2010, which will be presented in the next post.</p>

  Posted by Michail Pavlis<div class="avatar-frame"><img src="/public/images/michailpavlis.jpg"></img></div>
</div>

    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'alexandersingleton'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/talk/2014/05/11/AAG-Geodemographic-Update/">
            Updating OAC - Assessing Classification Uncertainty and Cluster Stability During Intercensal Periods
            <small>11 May 2014</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/talk/2014/05/11/AAG-Built-Environment/">
            Establishing Dynamic Measures of Built Environment Characteristics and their Relationship to Patterns of Socio-Spatial Structure
            <small>11 May 2014</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/talk/2014/05/10/AAG/">
            Geographic Data Science at the AAG
            <small>10 May 2014</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

      <HR>
    </div>


<!-- Comment Counts -->
<script type="text/javascript">
		var disqus_shortname = 'alexandersingleton'; // required: replace example with your forum shortname
                var disqus_identifier = '/r/2014/01/24/small-area-estimation-in-r-pt1/';
		var disqus_url = 'http://www.geographicdatascience.com/r/2014/01/24/small-area-estimation-in-r-pt1/';
 
		/* * * DON'T EDIT BELOW THIS LINE * * */
		(function () {
			var s = document.createElement('script'); s.async = true;
			s.type = 'text/javascript';
			s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
			(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
		}());
	</script>

  </body>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50923993-1', 'geographicdatascience.com');
  ga('send', 'pageview');

</script>

</html>
