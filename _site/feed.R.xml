<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Geographic Data Science Laboratory - R</title>
		<description>Posts categorized as 'R'</description>
		<link>http://www.geographicdatascience.com</link>
		<atom:link href="http://www.geographicdatascience.com/feed.R.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Small Area Estimation in R - Part 2</title>
				<description>&lt;p&gt;To continue from the previous post, I will show how to make predictions of people in Routine/Semiroutine occupations for the period 2002 - 2010 using the GLMM model which was presented in the previous post. The predictions will be made using both the fixed effects and the random effects of the model (at the LSOA level), which will result in what is known as the empirical best linear unbiased prediction (EBLUP). &lt;!-- more --&gt;&lt;/p&gt;

&lt;p&gt;To carry out the analysis we will need to update the data for the fixed variables, i.e. rank education and (transformed) Band A percent. The data for Tax Bands for the period 2002 - 2010 can be found &lt;a href=&quot;http://www.neighbourhood.statistics.gov.uk/dissemination/datasetList.do?JSAllowed=true&amp;amp;Function=&amp;amp;%24ph=60&amp;amp;CurrentPageId=60&amp;amp;step=1&amp;amp;CurrentTreeIndex=-2&amp;amp;searchString=tax+bands&amp;amp;datasetFamilyId=938&amp;amp;Next.x=13&amp;amp;Next.y=7&quot;&gt;here&lt;/a&gt;, as explained previously, apart from the year we need to select 9 regions (East Midlands, East of England, London, North East, North West, South East, South West, West England and Yorkhire), the type of file (csv) and extract the file ending in &quot;_OA.CSV&quot;. Before extracting the files you should create a base folder and within that folder create 9 more folders named after each year (2002 - 2010). Different OA codes are used from 2010 so we will need a &lt;a href=&quot;https://geoportal.statistics.gov.uk/Docs/Lookups/Output_areas_(2001)_to_output_areas_(2011)_to_local_authority_districts_(2011)_E+W_lookup.zip&quot;&gt;lookup table&lt;/a&gt; between the 2001 and 2010 OA codes. Updates for the indicator of education deprivation are created every 3 years, so for 2007 can be found &lt;a href=&quot;http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&amp;amp;Function=&amp;amp;%24ph=60_61&amp;amp;CurrentPageId=61&amp;amp;step=2&amp;amp;datasetFamilyId=2026&amp;amp;instanceSelection=123850&amp;amp;Next.x=11&amp;amp;Next.y=10&quot;&gt;here&lt;/a&gt; and for 2010 &lt;a href=&quot;http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&amp;amp;Function=&amp;amp;%24ph=60_61&amp;amp;CurrentPageId=61&amp;amp;step=2&amp;amp;datasetFamilyId=2394&amp;amp;instanceSelection=129711&amp;amp;Next.x=20&amp;amp;Next.y=13&quot;&gt;here&lt;/a&gt;. For the years that the deprivation data are not available we will keep them constant.&lt;/p&gt;

&lt;p&gt;We will first need to merge the tax bands data for each year. This can be accomplished with the following function, simply provide the path to the base folder.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;mergeTaxData &amp;lt;- function(path_to_base_folder){
  for (year in 2002:2010){ 
    setwd(paste(path_to_base_folder, year, sep = &quot;/&quot;))
    csv_files &amp;lt;- list.files(&quot;.&quot;, &quot;*.CSV&quot;)
    temp &amp;lt;- NULL
    if (year &amp;lt; 2008){
      cols &amp;lt;- c(12, 15, 16, 18, 20, 22, 24, 26, 28, 30)
      } else {
      cols &amp;lt;- c(14, 17, 18, 20, 22, 24, 26, 28, 30, 32)
      }
    for (i in csv_files){
      read_csv &amp;lt;- read.csv(i, skip = 5, header = T)[,cols]
      temp &amp;lt;- rbind(temp, read_csv)
    }
    if (year != 2010){
        names(temp) &amp;lt;- c(&quot;oa_01&quot;, &quot;all_bands&quot;, &quot;band_a&quot;, &quot;band_b&quot;, &quot;band_c&quot;, &quot;band_d&quot;, &quot;band_e&quot;, &quot;band_f&quot;, &quot;band_g&quot;, &quot;band_h&quot;)
        } else {
          names(temp) &amp;lt;- c(&quot;oa_11&quot;, &quot;all_bands&quot;, &quot;band_a&quot;, &quot;band_b&quot;, &quot;band_c&quot;, &quot;band_d&quot;, &quot;band_e&quot;, &quot;band_f&quot;, &quot;band_g&quot;, &quot;band_h&quot;)
        }
    write.csv(temp, paste(&quot;tax_bands_&quot;,year,&quot;.csv&quot;,sep=&quot;&quot;), row.names = F)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So if the path to the base folder was C:\tax_bands then the function could be used as follows:
&lt;code&gt;{r exampleFun, eval = FALSE}
mergeTaxData(&quot;C:/tax_bands&quot;)
&lt;/code&gt;
We could also check if it is possible to improve our model. For example if spatial autocorrelation was found in the residuals we could take advantage of that and krige the residuals to add the predicted values back to our predictions (from the GLMM model), a methodology which is known as rgeression kriging. We can evaluate if there is autocorrelation in the residuals using the variogram, but first we will need the coordinates of the centroids of the OAs. To do that, download the OA boundaries from the site of &lt;a href=&quot;http://ukbsrv-at.edina.ac.uk/html/easy_download/easy_download.html?data=England_oa_2001&quot;&gt;Edina&lt;/a&gt; (you will need to login to access the data). The following script will extract the coordinates and draw the variogram for the dependent variable and the residuals obtained from the GLMM model. If the curve of the variogram is increasing with distance then there is autocorrelation in the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;library(rgdal)
library(gstat)

oa_boundaries &amp;lt;- readOGR(&quot;/home/mick/GIS_Data&quot;, &quot;England_oa_2001_clipped_area&quot;)
oa_coords &amp;lt;- coordinates(oa_boundaries) # Extract the coordinates of the centroids of the OA polygons
oa_coords &amp;lt;- data.frame(oa_01 = oa_boundaries@data[,2], easting = oa_coords[,1], northing = oa_coords[,2])
all_data2 &amp;lt;- merge(all_data, oa_coords, by.x = &quot;oa_01&quot;, by.y = &quot;oa_01&quot;)
coordinates(all_data2) &amp;lt;- ~ easting + northing # Convert to SpatialPointsDataFrame
vgm1 &amp;lt;- variogram(routine_perc_01 ~ 1, all_data2)
vgm2 &amp;lt;- variogram(E3 ~ 1, all_data2)
par(mfrow = c(1,2))
plot(vgm1)
plot(vgm2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/vgm1.jpeg&quot; alt=&quot;vgm1&quot; /&gt;
&lt;img src=&quot;/public/images/vgm2.jpeg&quot; alt=&quot;vgm2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As can be seen from the variograms, autocorrelation was present in the dependent variable but it was accounted for after regression was applied. So the next step of the analysis, is to make predictions for the period 2002 - 2010, which can be accomplished using the following script.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;out &amp;lt;- data.frame(oa_01 = all_data$oa_01, routine_perc_01 = all_data$routine_perc_01)

lookup2 &amp;lt;- read.csv(&quot;path/to/lookup_01_11.csv&quot;)[,2:3]
names(lookup2) &amp;lt;- c(&quot;oa_01&quot;, &quot;oa_11&quot;)

for (year in 2002:2010){

  if (year &amp;lt;= 2004){
    imd_education &amp;lt;- read.csv(&quot;path/to/IMD_Education_2004.csv&quot;, skip = 5)[,c(9,14)]
    names(imd_education) &amp;lt;- c(&quot;LSOA01CD&quot;, &quot;rank_education_x&quot;)
  } else if (year &amp;gt; 2004 &amp;amp; year &amp;lt; 2008){
    imd_education &amp;lt;- read.csv(&quot;path/to//IMD_Education_2007.csv&quot;, skip = 5)[,c(9,14)]
    names(imd_education) &amp;lt;- c(&quot;LSOA01CD&quot;, &quot;rank_education_x&quot;)
    } else {
      imd_education &amp;lt;- read.csv(&quot;path/to/IMD_Education_2010.csv&quot;, skip = 5)[,c(11,14)]
      names(imd_education) &amp;lt;- c(&quot;LSOA01CD&quot;, &quot;rank_education_x&quot;)
    }

  bands &amp;lt;- read.csv(paste(&quot;/home/mick/dwelling/&quot;, year, &quot;/tax_bands_&quot;, year,&quot;.csv&quot;, sep = &quot;&quot;))
  bands$a_perc_x &amp;lt;- ifelse(bands$bands_all  != 0, bands$band_a / bands$bands_all, 0)

  if (year == 2010){
    bands &amp;lt;- merge(bands, lookup2, by.x = &quot;oa_11&quot;, by.y = &quot;oa_11&quot;)
  }

  temp &amp;lt;- merge(all_data[,c(&quot;oa_01&quot;, &quot;LSOA01CD&quot;, &quot;a_perc&quot;, &quot;rank_education&quot;)], 
                bands[,c(&quot;oa_01&quot;, &quot;a_perc_x&quot;)], all.x = T, by.x = &quot;oa_01&quot;, by.y = &quot;oa_01&quot;)
  temp &amp;lt;- merge(temp, imd_education, all.x = T, by.x = &quot;LSOA01CD&quot;, by.y = &quot;LSOA01CD&quot;)

  # Replace any NAs
  temp$a_perc_x &amp;lt;- as.numeric(ifelse(is.na(temp$a_perc_x), temp$a_perc, temp$a_perc_x))
  temp$rank_education_x &amp;lt;- as.numeric(ifelse(is.na(temp$rank_education_x), temp$rank_education, temp$rank_education_x))

  temp$tr_a_perc_x &amp;lt;- asin(sqrt(temp$a_perc_x/(100 + 1))) + asin(sqrt((temp$a_perc_x + 1)/(100 + 1))) 

  X &amp;lt;- with(temp, data.frame(tr_a_perc =  tr_a_perc_x, rank_education = rank_education_x, LSOA01CD = LSOA01CD))

  pred_name &amp;lt;- paste(&quot;pred_routine_&quot;, year, sep = &quot;&quot;)
  temp[[pred_name]] &amp;lt;- predict(glm3, X, REform =~ (1|LSOA01CD))
  temp[[pred_name]] &amp;lt;- exp(temp[[pred_name]]) / (1 + exp(temp[[pred_name]]))

  out &amp;lt;- merge(out, temp[,c(&quot;oa_01&quot;, pred_name)], all.x = T, by.x = &quot;oa_01&quot;, by.y = &quot;oa_01&quot;)
  out[[pred_name]] &amp;lt;- as.numeric(ifelse(is.na(out[[pred_name]]), out$routine_perc_01, out[[pred_name]]))
}

write.csv(out, &quot;pred_routine.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can have a look at the correlations between the Census 2001 data and the predicted values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;cor(out[,2:11])
#                   routine_perc_01 pred_routine_2002 pred_routine_2003 pred_routine_2004 pred_routine_2005 pred_routine_2006 pred_routine_2007 pred_routine_2008 pred_routine_2009 pred_routine_2010
# routine_perc_01         1.0000000         0.8711056         0.8710991         0.8711549         0.8655008         0.8654215         0.8654712         0.8616898         0.8524694         0.8614698
# pred_routine_2002       0.8711056         1.0000000         0.9998875         0.9994646         0.9882453         0.9882038         0.9882186         0.9811680         0.9682729         0.9806596
# pred_routine_2003       0.8710991         0.9998875         1.0000000         0.9995565         0.9883512         0.9883136         0.9883332         0.9812892         0.9683978         0.9807874
# pred_routine_2004       0.8711549         0.9994646         0.9995565         1.0000000         0.9887782         0.9887463         0.9887642         0.9817424         0.9687370         0.9811330
# pred_routine_2005       0.8655008         0.9882453         0.9883512         0.9887782         1.0000000         0.9997909         0.9994789         0.9885431         0.9757190         0.9880140
# pred_routine_2006       0.8654215         0.9882038         0.9883136         0.9887463         0.9997909         1.0000000         0.9997002         0.9887677         0.9759328         0.9881429
# pred_routine_2007       0.8654712         0.9882186         0.9883332         0.9887642         0.9994789         0.9997002         1.0000000         0.9890717         0.9762212         0.9884217
# pred_routine_2008       0.8616898         0.9811680         0.9812892         0.9817424         0.9885431         0.9887677         0.9890717         1.0000000         0.9871076         0.9993604
# pred_routine_2009       0.8524694         0.9682729         0.9683978         0.9687370         0.9757190         0.9759328         0.9762212         0.9871076         1.0000000         0.9868587
# pred_routine_2010       0.8614698         0.9806596         0.9807874         0.9811330         0.9880140         0.9881429         0.9884217         0.9993604         0.9868587         1.0000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As can be seen from the correlation analysis, the correlation between the Census data and the predicted values is quite good and the correlation decreases over time as should be expected.&lt;/p&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Using a GLMM it was possible to make predictions for the percentage of people in routine semi-routine occupation, internal validation based on the residuals was used to evaluate the outcome of the analysis. Even though there is a good correlation between the Census data and the predictions, there is a stronger correlation between the predictions themselves which might indicate that we don&#39;t predict some of the changes that occur over time. A more elegant solution to the problem of overdispersion would be to apply Baysian statistics and to use a different distribution from binomial (e.g. beta-binomial).&lt;/p&gt;
</description>
				<pubDate>Thu, 06 Feb 2014 00:00:00 +0000</pubDate>
				<link>http://www.geographicdatascience.com//r/2014/02/06/small-area-estimation-in-r-pt2/</link>
				<guid isPermaLink="true">http://www.geographicdatascience.com//r/2014/02/06/small-area-estimation-in-r-pt2/</guid>
			</item>
		
			<item>
				<title>Small Area Estimation in R - Part 1</title>
				<description>&lt;p&gt;A substantial part of my work has to do with statistics and mostly in the area of geodemographics and &lt;a href=&quot;http://en.wikipedia.org/wiki/Small_area_estimation&quot;&gt;Small Area Estimation&lt;/a&gt; (SAE). Basically what I am trying to do is to update the &lt;a href=&quot;http://areaclassification.org.uk/getting-started/getting-started-what-is-the-output-area-classification/&quot;&gt;Output Area Classification&lt;/a&gt; (OAC) for England by producing intercensal estimates of the OAC variables.  Some of these estimates are already available, such as population estimates, but for the most part we have to create statistical models using secondary data sources such as school data, council tax bands, indices of deprivation and so on. &lt;!-- more --&gt;&lt;/p&gt;

&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt;There are two major distinctions in regards with the statistical methods used in SAE, the direct and indirect estimators. The former makes use of domain-specific data and design-based weights while the latter  mostly rely on the use of auxiliary data. Further distinctions of indirect estimators can be made by identifying whether they “borrow strength” from a different domain, time or a combination of the two. Indirect estimators are also called model-based estimators and use techniques such as mixed-effects models and the empirical best linear unbiased prediction (EBLUP), hierarchical Bayes and empirical Bayes. A wealth of information on SAE can be found in the book Small Area Estimation by Rao.&lt;/p&gt;

&lt;p&gt;Mixed-effects models are applied to data which are nested (also referred to as hierarchical or multilevel). Geodemographic data are nested because there is a spatial hierarchy, for example output areas are within lower super output areas. This hierarchy can be included in the random part of the model while the fixed part of the model consists of the explanatory variables. More information on mixed-effects models can be found &lt;a href=&quot;http://zoonek2.free.fr/UNIX/48_R/14.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My own background is in the area of environmental sciences and I had the opportunity to use mixed-effects models, relying mostly on the excellent book by Zuur et al “Mixed effects Models and extentions in ecology using R”.  I have used in the past several packages in R to develop mixed effects models, including nlme, mgcv and lme4. There are several options for Bayesian analysis in R as well, but what I quickly found out was that running a Bayesian model at the national scale can take an enormous amount of time. This makes difficult to test and validate these models so in my case it was impossible to consider using Bayesian models.&lt;/p&gt;

&lt;h2&gt;Example&lt;/h2&gt;

&lt;p&gt;The type of model that I used was dictated by the type of data that I was interested to predict. Most of the variables that were used to develop the OAC were proportional data and therefore a generalized mixed-effects model (GLMM) with a binomial distribution had to be applied for their prediction (having said that, it has been suggested that a linear mixed effects model sometimes can provide better results, see for example this &lt;a href=&quot;http://www.google.co.uk/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=3&amp;amp;ved=0CDQQFjAC&amp;amp;url=http%3A%2F%2Fwww.researchgate.net%2Fpublication%2F256972075_Use_of_Spatial_Information_in_Small_Area_Models_for_Unemployment_Rate_Estimation_at_Sub-Provincial_Areas_in_Italy%2Ffile%2F504635241a2a1c8159.pdf&amp;amp;ei=fsvnUqWSENSB7Qab-oHYCw&amp;amp;usg=AFQjCNECEufo24VHT_f3AYyJi9I1JfvLoA&amp;amp;bvm=bv.59930103,d.ZGU&quot;&gt;paper&lt;/a&gt;). lme4 is a good package to start exploring GLMM models in R and I will provide an example of using lme4 later in this post. Aspects of the analysis that one should pay particular attention include:
  * Exploratory Data Analysis (identification of suitable predictors, outliers, non-linearity).
  * Model Selection for the fixed part and the random part of the regression.
  * Internal validation (based on the residuals).
  * Overdispersion (i.e. the variation in the data exceeds the expected variability based on the binomial distribution assumptions).&lt;/p&gt;

&lt;p&gt;If you want to reproduce the analysis you will have to download the 2001 Census data on &lt;a href=&quot;http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&amp;amp;Function=&amp;amp;%24ph=60_61&amp;amp;CurrentPageId=61&amp;amp;step=2&amp;amp;datasetFamilyId=41&amp;amp;instanceSelection=121&amp;amp;Next.x=16&amp;amp;Next.y=5&quot;&gt;Routine/Semiroutine Occupation&lt;/a&gt;. So download the files for the following regions: East Midlands, East of England, London, North East, North West, South East, South West, West England and Yorkhire (9 files in total). Select csv as file type, download the zip file in a folder and then unzip the file ending in &quot;_OA.CSV&quot;. The following R script will select the appropriate columns, merge the data and save them in a new csv file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;cols = c(12, 15, 28, 30)
setwd(&quot;path/to/census_data&quot;)  # Provide the path to Census 2001 csv files
csv_files = list.files(&quot;.&quot;, &quot;.CSV&quot;)
temp &amp;lt;- NULL
for (i in csv_files) {
    pop &amp;lt;- read.csv(i, skip = 5, header = T)
    pop &amp;lt;- pop[, cols]
    names(pop) &amp;lt;- c(&quot;oa&quot;, &quot;all_people&quot;, &quot;semi_routine_occupation&quot;, &quot;routine_occupation&quot;)
    temp &amp;lt;- rbind(temp, pop)
}
out &amp;lt;- data.frame(oa_01 = temp[, 1], all_people_01 = temp[, 2])
out$routine_count_01 &amp;lt;- rowSums(temp[, 3:4])
out$routine_perc_01 &amp;lt;- rowSums(temp[, 3:4])/temp[, 2]

write.csv(out, &quot;occupation_2001.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Consequently download the &lt;a href=&quot;https://geoportal.statistics.gov.uk/Docs/Lookups/Output_areas_(2001)_to_lower_layer_super_output_areas_(2001)_to_middle_layer_super_output_areas_(2001)_E+W_lookup.zip&quot;&gt;lookup table&lt;/a&gt; between OA 2001 and LSOA and unzip the csv file. You will also need the Indices of Deprivation 2004 (Education), so go ahead and download it &lt;a href=&quot;http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&amp;amp;Function=&amp;amp;%24ph=60_61&amp;amp;CurrentPageId=61&amp;amp;step=2&amp;amp;datasetFamilyId=2088&amp;amp;instanceSelection=124670&amp;amp;Next.x=13&amp;amp;Next.y=18&quot;&gt;here&lt;/a&gt;. Again select csv as data type and from the zipped file export the file ending to &quot;_LSOA.CSV&quot;. We will also need the council tax bands data for 2001, which you can find &lt;a href=&quot;http://www.neighbourhood.statistics.gov.uk/dissemination/datasetList.do?JSAllowed=true&amp;amp;Function=&amp;amp;%24ph=60&amp;amp;CurrentPageId=60&amp;amp;step=1&amp;amp;CurrentTreeIndex=-2&amp;amp;searchString=tax+bands&amp;amp;datasetFamilyId=938&amp;amp;Next.x=13&amp;amp;Next.y=7&quot;&gt;here&lt;/a&gt;. Like the 2001 census data, tax bands are available for each region so save them in a new folder and then merge them to a single file using the following script.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;setwd(&quot;path/to/tax_bands_data&quot;)  # Provide the path to tax bands data
csv_files &amp;lt;- list.files(&quot;.&quot;, &quot;*.CSV&quot;)
temp &amp;lt;- NULL
cols &amp;lt;- c(12, 15, 16, 18, 20, 22, 24, 26, 28, 30)
for (i in csv_files) {
    read_csv &amp;lt;- read.csv(i, skip = 5, header = T)[, cols]
    names(read_csv) &amp;lt;- c(&quot;oa_01&quot;, &quot;all_bands&quot;, &quot;band_a&quot;, &quot;band_b&quot;, &quot;band_c&quot;, 
        &quot;band_d&quot;, &quot;band_e&quot;, &quot;band_f&quot;, &quot;band_g&quot;, &quot;band_h&quot;)
    temp &amp;lt;- rbind(temp, read_csv)
}

write.csv(temp, &quot;tax_bands_2001.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we can put all the data together and start with the analysis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;library(lme4)
source(file = &quot;http://www.highstat.com/Book2/HighstatLibV6.R&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;census_01 &amp;lt;- read.csv(&quot;path/to/occupation_2001.csv&quot;)[, 1:4]

lookup &amp;lt;- read.csv(&quot;path/to/OA01_LSOA01_MSOA01_EW_LU.csv&quot;)[, 1:2]

imd_education &amp;lt;- read.csv(&quot;path/to/IMD_Education_2004.csv&quot;, skip = 5)[, c(9,14)]
names(imd_education) &amp;lt;- c(&quot;LSOA01CD&quot;, &quot;rank_education&quot;)

bands_01 &amp;lt;- read.csv(&quot;path/to/tax_bands_2001.csv&quot;)
bands_01$a_perc &amp;lt;- ifelse(bands_01$all_bands == 0, 0, bands_01$band_a/bands_01$all_bands)

census_01 &amp;lt;- merge(census_01, lookup, by.x = &quot;oa_01&quot;, by.y = &quot;OA01CD&quot;)
all_data &amp;lt;- merge(census_01, bands_01[, c(&quot;oa_01&quot;, &quot;a_perc&quot;)], by.x = &quot;oa_01&quot;, 
    by.y = &quot;oa_01&quot;)
all_data &amp;lt;- merge(all_data, imd_education[, c(&quot;LSOA01CD&quot;, &quot;rank_education&quot;)], 
    by.x = &quot;LSOA01CD&quot;, by.y = &quot;LSOA01CD&quot;)

all_data$tr_a_perc &amp;lt;- asin(sqrt(all_data$a_perc/(100 + 1))) + asin(sqrt((all_data$a_perc + 1)/(100 + 1)))

str(all_data)
# &#39;data.frame&#39;: 165665 obs. of 9 variables: 
# $ LSOA01CD : Factor w/ 34378 levels &#39;E01000001&#39;,&#39;E01000002&#39;,..: 1 1 1 1 1 1 1 1 2 2 ...  
# $ oa_01 : Factor w/ 165665 levels &#39;00AAFA0001&#39;,&#39;00AAFA0002&#39;,..: 1 2 3 4 5 6 7 8 17
# $ all_people_01 : int 181 92 180 246 206 197 164 148 199 189 ...
# $ routine_count_01: int 6 0 3 3 0 7 9 3 0 6 ...  
# $ routine_perc_01 : num 0.0331 0 0.0167 0.0122 0 ...  
# $ a_perc : num 0 0 0 0 0 ...  
# $ rank_education : int 28215 28215 28215 28215 28215 28215 28215 28215 28452
# $ tr_a_perc : num 0.0997 0.0997 0.0997 0.0997 0.0997 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The response variable is routine_perc_01, while the explanatory variables are a_perc (Tax Band A) and rank_education. I should note that I prefer to use the Education Rank instead of Education Score as the former is ordinal data and we will have less problems with the regression analysis. I have also transformed the variable Band A using the Freeman - Tukey double arcsine transformation which is suitable for percentage data. You can see the effect of the transformation in the following QQ-plot, the main body of the data distribution is closer to normal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/qqplots.jpeg&quot; alt=&quot;QQplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can create the QQ-plot as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;par(mfrow = c(1, 2))
for (i in c(&quot;a_perc&quot;, &quot;tr_a_perc&quot;)) {
    qqnorm(all_data[[i]], main = i)
    qqline(all_data[[i]])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Exploratory Data Analysis&lt;/h3&gt;

&lt;p&gt;The next step is to perform EDA and we can start with looking at the correlations among the variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;cor(all_data[,c(5,6:8)])
#                 routine_perc_01     a_perc rank_education  tr_a_perc
# routine_perc_01       1.0000000  0.5894138     -0.7334502  0.5974160
# a_perc                0.5894138  1.0000000     -0.6021874  0.9795349
# rank_education       -0.7334502 -0.6021874      1.0000000 -0.5994592
# tr_a_perc             0.5974160  0.9795349     -0.5994592  1.0000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Education rank seems to be a good predictor and Band A is also related to the dependent variable.
We can check for outliers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;vars &amp;lt;- c(&quot;a_perc&quot;, &quot;tr_a_perc&quot;, &quot;rank_education&quot;)
Mydotplot(all_data[, c(vars, &quot;routine_perc_01&quot;)])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/dotplot.jpeg&quot; alt=&quot;dotplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And visualize the relationship between the variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;Myxyplot(all_data, vars, &quot;routine_perc_01&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/xyplot.jpeg&quot; alt=&quot;xyplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The functions Mydotplot and Myxyplot are available from the HighstatLibV6.R script that we imported earlier. Outliers do not appear to be a significant problem for our data and the relationships seem to be linear.&lt;/p&gt;

&lt;h3&gt;Regression Analysis&lt;/h3&gt;

&lt;p&gt;Before we start with regression analysis we need to create the response variable which should consist of two columns, the number of people working in a routine occupation and the number of people who don&#39;t. We will also use the LSOA code in the random part of the regression. We will first use the untransformed percentage of Band A. The code in R is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;y &amp;lt;- cbind(all_data$routine_count_01, all_data$all_people_01 - all_data$routine_count_01)

glm1 &amp;lt;- glmer(y ~ a_perc + rank_education + (1 | LSOA01CD), data = all_data, 
    family = binomial)

summary(glm1)
# Generalized linear mixed model fit by maximum likelihood [&#39;glmerMod&#39;]
# Family: binomial ( logit )
# Formula: y ~ a_perc + rank_education + (1 | LSOA01CD) 
#   Data: all_data 

#       AIC       BIC    logLik  deviance 
# 1428001.9 1428041.9 -713996.9 1427993.9 

# Random effects:
#  Groups   Name        Variance Std.Dev.
#  LSOA01CD (Intercept) 0.08882  0.298   
# Number of obs: 165665, groups: LSOA01CD, 32482

# Fixed effects:
#                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
# (Intercept)    -9.541e-01  3.725e-03  -256.1   &amp;lt;2e-16 ***
# a_perc          5.367e-01  2.492e-03   215.3   &amp;lt;2e-16 ***
# rank_education -3.607e-05  1.912e-07  -188.6   &amp;lt;2e-16 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# Correlation of Fixed Effects:
#             (Intr) a_perc
# a_perc      -0.404       
# rank_eductn -0.873  0.287
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All the parameters are significant, the next step is to perform internal validation of the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;# E1 &amp;lt;- residuals(glm1, type=&quot;pearson&quot;)
# summary(E1)
#       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
# -14.200000  -1.044000  -0.062490  -0.007915   0.957100  18.520000 
# p1 &amp;lt;- length(fixef(glm1)) + 1
# overdispersion1 &amp;lt;- sum(E1 ^ 2) / (nrow(all_data) - p1)  # 2.615604, should be around 1
# F1 &amp;lt;- fitted(glm1)
# summary(F1)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.01805 0.14610 0.20170 0.20960 0.26850 0.50770 
summary(all_data$routine_perc_01)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.0000  0.1366  0.2021  0.2094  0.2764  0.5666 
cor(all_data$routine_perc_01, F1)  # 0.8765911

par(mfrow = c(2, 1))
plot(F1, E1, xlab = &quot;Fitted values&quot;, ylab = &quot;Pearson residuals&quot;)
abline(h = 0, lty = 2)
plot(F1, all_data$routine_perc_01, xlab = &quot;Fitted values&quot;, ylab = &quot;Observed data&quot;)
abline(coef = c(0, 1), lty = 2)

all_data$E1 &amp;lt;- E1
vars &amp;lt;- c(&quot;a_perc&quot;, &quot;rank_education&quot;)
Myxyplot(all_data, vars, &quot;E1&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/glm1_plot1.jpeg&quot; alt=&quot;glm1_plot1&quot; /&gt;
&lt;img src=&quot;/public/images/glm1_plot2.jpeg&quot; alt=&quot;glm1_plot2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Based on the internal validation, the model is overdispersed and we don&#39;t predict extremely high and low values (nevertheless we do predict within the physical range). Plotting the Pearson residuals against predicted values there is greater variance in the middle of the graph compared to the variance on the right of the graph. The correlation of the predicted values against the actual values seems to be good. The plot of Pearson residuals against Band A shows that we have greater variance at the ends of the plot, therefore a transformation of the variable might be useful. Rank education data against the residuals does not show any serious patterns.
So the next step is to repeat the regression analysis using the Band A transformed data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;glm2 &amp;lt;- glmer(y ~ tr_a_perc + rank_education + (1 | LSOA01CD), data = all_data, 
    family = binomial)

summary(glm2)
# Generalized linear mixed model fit by maximum likelihood [&#39;glmerMod&#39;]
#  Family: binomial ( logit )
# Formula: y ~ tr_a_perc + rank_education + (1 | LSOA01CD) 
#    Data: all_data 

#       AIC       BIC    logLik  deviance 
# 1429994.9 1430035.0 -714993.5 1429986.9 

# Random effects:
#  Groups   Name        Variance Std.Dev.
#  LSOA01CD (Intercept) 0.08436  0.2904  
# Number of obs: 165665, groups: LSOA01CD, 32482

# Fixed effects:
#                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
# (Intercept)    -1.342e+00  4.744e-03  -282.9   &amp;lt;2e-16 ***
# tr_a_perc       3.622e+00  1.722e-02   210.3   &amp;lt;2e-16 ***
# rank_education -3.636e-05  1.870e-07  -194.5   &amp;lt;2e-16 ***
---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# Correlation of Fixed Effects:
#             (Intr) tr__pr
# tr_a_perc   -0.713       
# rank_eductn -0.787  0.291
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;E2 &amp;lt;- residuals(glm2, type = &quot;pearson&quot;)
summary(E2)
#      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
# -14.200000  -1.050000  -0.063270  -0.007156   0.963700  18.810000 
p2 &amp;lt;- length(fixef(glm2)) + 1
overdispersion2 &amp;lt;- sum(E2 ^ 2) / (nrow(all_data) - p2)  # 2.638352
F2 &amp;lt;- fitted(glm2)
summary(F2)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.01724 0.14590 0.20200 0.20960 0.26950 0.49210 
summary(all_data$routine_perc_01)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.0000  0.1366  0.2021  0.2094  0.2764  0.5666 
cor(all_data$routine_perc_01, F2)  # 0.8755668

par(mfrow = c(2, 1))
plot(F2, E2, xlab = &quot;Fitted values&quot;, ylab = &quot;Pearson residuals&quot;)
abline(h = 0, lty = 2)
plot(F2, all_data$routine_perc_01, xlab = &quot;Fitted values&quot;, ylab = &quot;Observed data&quot;)
abline(coef = c(0, 1), lty = 2)

all_data$E2 &amp;lt;- E2
vars &amp;lt;- c(&quot;tr_a_perc&quot;, &quot;rank_education&quot;)
Myxyplot(all_data, vars, &quot;E2&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/glm2_plot1.jpeg&quot; alt=&quot;glm2_plot1&quot; /&gt;
&lt;img src=&quot;/public/images/glm2_plot2.jpeg&quot; alt=&quot;glm2_plot2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With regard to the second model, the use of the transformed variable appears to have slightly increased the AIC, so one could argue against transforming BAND A. Nevertheless, the plot of residuals against the transformed variable shows that heterogeneity was decreased so I decided to use the transformed variable. The final step of the analysis is to deal with overdispersion and to do this a random intercept for each entry was included in the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;all_data$Eps &amp;lt;- 1:nrow(all_data)

glm3 &amp;lt;- glmer(y ~ tr_a_perc + rank_education + (1 | LSOA01CD) + (1 | Eps), data = all_data, 
    family = binomial)
summary(glm3)
# Generalized linear mixed model fit by maximum likelihood [&#39;glmerMod&#39;]
#  Family: binomial ( logit )
# Formula: y ~ tr_a_perc + rank_education + (1 | LSOA01CD) + (1 | Eps) 
#    Data: all_data 

#       AIC       BIC    logLik  deviance 
# 1286541.7 1286591.8 -643265.9 1286531.7 

# Random effects:
#  Groups   Name        Variance Std.Dev.
#  Eps      (Intercept) 0.07189  0.2681  
#  LSOA01CD (Intercept) 0.06475  0.2545  
# Number of obs: 165665, groups: Eps, 165665; LSOA01CD, 32482

# Fixed effects:
#                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
# (Intercept)    -1.263e+00  6.474e-03  -195.1   &amp;lt;2e-16 ***
# tr_a_perc       3.156e+00  2.849e-02   110.8   &amp;lt;2e-16 ***
# rank_education -3.806e-05  1.961e-07  -194.1   &amp;lt;2e-16 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# Correlation of Fixed Effects:
#             (Intr) tr__pr
# tr_a_perc   -0.867       
# rank_eductn -0.783  0.462

E3 &amp;lt;- residuals(glm3, type = &quot;pearson&quot;)
summary(E3)
#      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
# -4.024000 -0.329100  0.001431 -0.039470  0.295900  2.959000 
p3 &amp;lt;- length(fixef(glm3)) + 1
overdispersion3 &amp;lt;- sum(E3 ^ 2) / (nrow(all_data) - p3)  # 0.2939502

all_data$E3 &amp;lt;- E3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that overdispersion for the last model is no longer a problem. The next part of the analysis is to predict new values of the response variable for the period 2002 - 2010, which will be presented in the next post.&lt;/p&gt;
</description>
				<pubDate>Fri, 24 Jan 2014 00:00:00 +0000</pubDate>
				<link>http://www.geographicdatascience.com//r/2014/01/24/small-area-estimation-in-r-pt1/</link>
				<guid isPermaLink="true">http://www.geographicdatascience.com//r/2014/01/24/small-area-estimation-in-r-pt1/</guid>
			</item>
		
			<item>
				<title>Reprojecting Crime data from Police.uk from Latitude &amp; Longitude (WGS84) to Eastings &amp; Northings (British National Grid)</title>
				<description>&lt;p&gt;I have recently taken over a second year undergraduate module, &#39;Applied GIS and Modelling&#39;. The first practical element of the course (spread over 8 hours of lab sessions over the first 2 weeks) introduces the students to GIS and using R to show, create and handle spatial data. One of the exercises uses crime data downloaded from the Police.uk website.&lt;/p&gt;

&lt;p&gt;While going through the material, I discovered that the crime data available through the Police.uk website had changed format. &lt;!-- more --&gt; The data file used last year previously contained coordinates in Eastings and Northings (the &lt;a href=&quot;http://www.ordnancesurvey.co.uk/resources/maps-and-geographic-resources/the-national-grid.html&quot;&gt;British National Grid&lt;/a&gt; projection system) now had coordinates specified in Latitude and Longitude (&lt;a href=&quot;http://spatialreference.org/ref/epsg/4326/&quot;&gt;WGS84&lt;/a&gt;). The students go on to plot the crime data over some LSOA boundaries (which come projected in BNG) so it would be useful to have the crime data in the same projection. This is in the first practical, so while the students do need to learn about projections at some point, I would prefer not to have to throw them in at the deep end to begin with!&lt;/p&gt;

&lt;p&gt;However, the data had changed, so I needed to find a simple way of reprojecting them. Fortunately with a few tweaks to the R code, I managed to incorporate this into the practical in a way that wasn&#39;t too complex. The full practical is available at &lt;a href=&quot;http://rpubs.com/nickbearman/gettingstartedwithr&quot;&gt;http://rpubs.com/nickbearman/gettingstartedwithr&lt;/a&gt;, and the bit relevant to this post begins about half-way down, at &#39;Working with Open Government Data&#39;. The steps to download the crime data from Police.uk are straight forward, and are covered in the practical. The students use data from Merseyside Police, as this is their local police area. This is all done using the MapTools library in R.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;    #Load the library
        require(maptools) 
    #Read in the data
        crimes &amp;lt;- read.csv(&quot;2013-11-merseyside-street.csv&quot;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step reads the CSV data (with latitude and longitude fields) into a SpatialPointsDataFrame:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;    #Show the head of the data
        head(crimes)
    #Create the coordinates
        coords &amp;lt;- cbind(Longitude = as.numeric(as.character(crimes$Longitude)), Latitude = as.numeric(as.character(crimes$Latitude))) 
    #Create SpatialPointsDataFrame, specifing coordinates, data and projection
        crime.pts &amp;lt;- SpatialPointsDataFrame(coords, crimes[, -(5:6)], proj4string = CRS(&quot;+init=epsg:4326&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the practical:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;This creates a SpatialPointsDataFrame object. This first line prepares the coordinates into a form that the SpatialPointsDataFrame can use. The SpatialPointsDataFrame function on the second line takes three arguments - the first is coordinates, created in the line above. The second argument is the data frame minus columns 5 and 6 - this is what &lt;code&gt;-(5:6)&lt;/code&gt; indicates. The third argument is the projection. These columns provide all the non-geographical data from the data frame. The resulting object crime.pts is a spatial points geographical shape object, whose points are each recorded crime in the data set you download.&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Creating a SpatialPointsDataFrame from the CSV is straight forward, but remember to specify the projection - &lt;code&gt;proj4string = CRS(&quot;+init=epsg:4326&quot;)&lt;/code&gt; in our case (WGS84). The &lt;code&gt;[,-(5:6)]&lt;/code&gt; means that the data element of the SpatialPointsDataFrame contains everything in crimes (the CSV file read in) apart from columns 5 &amp;amp; 6. This is a new way of doing it to me, but quite effective!&lt;/p&gt;

&lt;p&gt;Once the data is stored as a SpatialPointsDataFrame, the actual reprojection is easy - you essentially just say &#39;reproject this data frame to this projection&#39;, with &lt;code&gt;+init=epsg:27700&lt;/code&gt; being BNG:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;    #Reproject data to BNG (27700)  
        crime.pts &amp;lt;- spTransform(crime.pts, CRS(&quot;+init=epsg:27700&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As with many things in R, if it&#39;s setup correctly, then a reprojection is quite easy to do (just one line in this example!). However, it is setting up the data correctly which takes time. Previously both the shape file of the LSOAs and the crime data were read in without any projection - fine if they are both the same, but to reproject it, both need their projections specifying.&lt;/p&gt;

&lt;p&gt;Hopefully this R code will be useful to you - feel free to use it in any of your work.&lt;/p&gt;
</description>
				<pubDate>Mon, 20 Jan 2014 00:00:00 +0000</pubDate>
				<link>http://www.geographicdatascience.com//r/2014/01/20/reprojecting-crime-data/</link>
				<guid isPermaLink="true">http://www.geographicdatascience.com//r/2014/01/20/reprojecting-crime-data/</guid>
			</item>
		
	</channel>
</rss>
